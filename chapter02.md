# 第2章 AIフレームワークとの関係を理解する
### 2.1 トレーニングと推論の分離
ディープラーニングの開発では「トレーニング（学習）」と「推論（利用）」を別の環境で行うケースが増えています。学習では大量のデータを読み込み、何度も計算を繰り返すため、GPUを備えたワークステーションやクラウドGPUが必要です。一方、推論はユーザーのデバイスやサーバーで素早く結果を返すことが目的であり、必ずしも同じフレームワークを使う必要はありません。

ONNXはこの分離をスムーズにします。PyTorchやTensorFlowで学習したモデルを一度ONNX形式に変換しておけば、推論側ではONNX Runtimeや他のエンジンを自由に選べます。例えば、研究チームがPyTorchで最新モデルを作り、製品開発チームがONNX Runtimeを使ってスマートフォンアプリに組み込む、といったワークフローが現実的になります。

### 2.2 フレームワークごとのONNXサポート状況
主要フレームワークはONNXへのエクスポート機能を備えています。PyTorchでは`torch.onnx.export`を使い、学習済みモデルとダミーの入力データを渡すだけで基本的な変換が完了します。TensorFlowの場合は公式ツールとして`tf2onnx`が提供されており、SavedModelやKerasモデルをONNXへ変換できます。MXNet、Chainer、Scikit-learnなどもコミュニティ製ツールで対応しており、ONNXのエクスポート方法はオンラインドキュメントやGitHubで常に更新されています。

サポート状況は日々変化するため、「ONNX フレームワーク名 export」などで検索すると最新情報が得られます。変換が難しいケースでも、開発コミュニティに質問したり、バージョンを合わせたりすることで解決できることが多いです。

### 2.3 ONNXとONNX Runtimeの関係
ONNXはあくまでファイル形式と仕様であり、それ自体が推論を実行するわけではありません。推論を担うのがONNX Runtimeです。ONNX RuntimeはMicrosoftが中心となって開発する高速な推論エンジンで、CPU、GPU、DirectML、TensorRTなどさまざまな実行プロバイダ（Execution Provider）を切り替えて利用できます。

ONNXモデルはONNX Runtime以外のエンジンでも利用できます。NVIDIAのTensorRTはGPU推論を最大限高速化するツールで、ONNX入力から最適化されたエンジンを生成できます。IntelのOpenVINOやAppleのCore ML Toolsも同様です。用途やハードウェアに合わせて最適な推論エンジンを選べる点が、ONNXエコシステムの大きな魅力です。

## 図のアイデア
- images/ch02_training_inference_flow.png — 学習環境から推論環境へのワークフロー図

## 演習
1. 任意のフレームワークで学習したモデルを列挙し、それぞれがONNXへ変換できるか調査する
2. ONNX Runtime以外の推論エンジンを2つ選び、利用したい理由と懸念点を整理する
