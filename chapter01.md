# 第1章 ONNXとは何か―歴史と特徴
### 1.1 ONNX誕生の背景
2010年代に入ってディープラーニングが一気に普及し、PyTorchやTensorFlow、MXNet、Chainerといった多様なフレームワークが登場しました。それぞれが独自のモデル形式を採用していたため、あるフレームワークで学習したモデルを別のフレームワークへ移すには、ほぼゼロから実装をやり直す必要がありました。研究発表で公開されたモデルを再利用するにも、利用者は「このモデルはどのフレームワークで学習されたのか？」を気にする必要があり、コラボレーションの障害になっていました。

こうした状況を改善しようと、2017年にMicrosoftとFacebook（現Meta）が中心となってONNXを提案しました。目的は、どのフレームワークで作ったモデルでも同じ形式で保存し、誰がどこで学習したモデルでも簡単に推論に利用できるようにすることです。その後、Amazon、NVIDIA、Intelなど多くの企業や研究機関が賛同し、コミュニティベースで標準規格として育まれてきました。

### 1.2 ONNXの目的と利点
ONNXの中心的な目的は「モデルの再利用性を高めること」です。ONNXファイルにはモデルを構成する演算（オペレーター）とテンソルの形状、重みが格納されます。これによって、PyTorchで学習したモデルをONNX形式に変換し、TensorFlowやC++アプリから読み込むといった柔軟なワークフローが実現します。

また、開発の現場では「学習はGPUを持つパワフルなマシンで、推論は低消費電力なエッジデバイスで」という使い分けが一般的になってきました。ONNXはこの分業体制にぴったりはまり、推論用の軽量なエンジン（ONNX Runtimeなど）へスムーズに引き渡せます。

パフォーマンス面でも利点があります。ONNX Runtimeは特定のハードウェア向けに最適化されたカーネルを用意しており、同じモデルでもフレームワークの純正推論より高速に動作することがあります。さらにONNXはオープン規格であるため、コミュニティが演算子を追加したり、最適化ツールを開発したりといった改善が継続的に行われています。

### 1.3 ONNXの進化
ONNXはバージョンごとに仕様（IR：Intermediate Representation）が定められており、互換性が慎重に扱われています。たとえば、古いバージョンで作成したモデルでも適切なツールを使えば新しいバージョンで読み込むことができ、逆もまた可能です。演算子（オペレーター）の追加や変更はONNXの仕様書に細かく記載され、コミュニティで議論したうえで採用されます。

対応エコシステムも年々拡大しています。主要なクラウドサービスはONNXをサポートしており、Azure Machine LearningやAWS SageMaker、Google Cloud Vertex AIなどでONNXモデルをそのまま推論に利用できます。NVIDIAのTensorRT、IntelのOpenVINO、AppleのCore MLなどハードウェアベンダーもONNXからの変換ツールを提供し、利用シナリオは急速に広がっています。本章を読み終えた時点で、「ONNXは将来性があり、幅広く使われている規格である」と理解できれば十分です。

## 図のアイデア
- images/ch01_timeline.png — ONNX誕生までの主要な出来事を年表で整理

## 演習
1. 主要なディープラーニングフレームワークとそのリリース年を調べ、ONNX誕生との関係をまとめる
2. ONNXの公式仕様書から最新のIRバージョンを確認し、追加された主な演算子を列挙する
